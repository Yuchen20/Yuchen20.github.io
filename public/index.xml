<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Yuchen&#39;s Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Yuchen&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Mar 2024 13:45:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About Me</title>
      <link>http://localhost:1313/posts/aboutme/</link>
      <pubDate>Fri, 08 Mar 2024 13:45:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/aboutme/</guid>
      <description>I am a master&amp;rsquo;s student pursuing an MPhil in Data Intensive Science at the University of Cambridge. My interests lie in computer vision, machine learning, and, particularly, in building efficient deep learning models. Previously, I studied Mathematics at the University of Edinburgh, where I had the privilege of working with Prof. ChengJia Wang on medical-related deep learning methods.
In my free time, I enjoy competing on Kaggle, reading Chinese fiction novels, and sometimes reading interesting papers on arXiv.</description>
    </item>
    <item>
      <title>Low Rank Adaptation and all its variansts</title>
      <link>http://localhost:1313/posts/low-rank-adaptation/</link>
      <pubDate>Fri, 08 Mar 2024 13:45:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/low-rank-adaptation/</guid>
      <description>This works is a wrapping up work that conclude all paper around LoRA I&amp;rsquo;ve read to develop SeLoRA. Note that the variants of LoRA included might not be comprehensive, and cut off around march 2024, but I hope this would give you an idea of what is LoRA, and what have already been accompolished in this.
Low Rank Adaptation Low Rank Adaptation (LoRA), a line of research under Parameter efficient Fine-Tuning (PEFT) methods, was initially designed for Large Language Models (LLMs), but has later been popularized by the Stable Diffusion community due to its fascinating ability of efficiently learning a style from only a few images.</description>
    </item>
  </channel>
</rss>
